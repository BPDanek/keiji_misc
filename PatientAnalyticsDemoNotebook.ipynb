{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BPDanek/keiji_misc/blob/main/PatientAnalyticsDemoNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuNihted-AQE"
      },
      "source": [
        "<center> <img src=\"data:image/svg+xml;base64,<svg width="1424" height="661" viewBox="0 0 1424 661" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect width="1424" height="661" rx="100" fill="#424242"/>
<rect width="204.12" height="204.12" rx="9" transform="matrix(0.866044 -0.499967 0.866044 0.499967 138 229.055)" stroke="white" stroke-width="35"/>
<rect width="122.472" height="122.472" rx="9" transform="matrix(-3.77048e-05 -1 -0.866007 -0.500033 312.534 459.879)" fill="white"/>
<rect width="204.12" height="204.12" rx="9" transform="matrix(-3.7675e-05 -1 -0.866007 -0.500033 312.157 541.379)" stroke="white" stroke-width="35"/>
<rect width="122.472" height="122.472" rx="9" transform="matrix(3.77048e-05 -1 0.866007 -0.500033 318.379 459.777)" fill="white"/>
<rect width="204.12" height="204.12" rx="9" transform="matrix(3.7671e-05 -1 0.866007 -0.500033 318.381 541.215)" stroke="white" stroke-width="35"/>
<path d="M608.25 407.5C601.583 407.5 598.25 405.667 598.25 402V237C598.25 233.333 601.583 231.5 608.25 231.5H610.25C616.917 231.5 620.25 233.333 620.25 237V313.5H621.25L682.75 235C683.917 233.667 685.75 232.75 688.25 232.25C690.917 231.75 693.583 231.5 696.25 231.5H698.75C704.583 231.5 707.5 232.583 707.5 234.75C707.5 236.25 706.667 237.917 705 239.75L642.5 317.75L718.25 400C719.75 401.667 720.5 403 720.5 404C720.5 406.333 716.833 407.5 709.5 407.5H706.5C703.833 407.5 701.25 407.333 698.75 407C696.25 406.833 694.25 405.917 692.75 404.25L621.25 323.75H620.25V402C620.25 405.667 616.917 407.5 610.25 407.5H608.25ZM788.16 408.5C767.493 408.5 752.077 403.083 741.91 392.25C731.91 381.417 726.91 364.833 726.91 342.5C726.91 327.667 729.077 315.25 733.41 305.25C737.91 295.25 744.493 287.667 753.16 282.5C761.827 277.333 772.493 274.75 785.16 274.75C796.66 274.75 806.16 277.25 813.66 282.25C821.327 287.25 827.077 294.583 830.91 304.25C834.743 313.917 836.66 325.833 836.66 340C836.66 341 836.16 342.333 835.16 344C834.327 345.5 833.243 346.917 831.91 348.25C830.577 349.583 829.41 350.25 828.41 350.25H749.41C750.41 364.083 753.993 374.25 760.16 380.75C766.327 387.25 775.577 390.5 787.91 390.5C795.243 390.5 801.41 389.667 806.41 388C811.577 386.167 815.66 384.417 818.66 382.75C821.66 380.917 823.577 380 824.41 380C825.577 380 826.827 380.833 828.16 382.5C829.493 384.167 830.66 386 831.66 388C832.66 390 833.16 391.5 833.16 392.5C833.16 394 831.993 395.667 829.66 397.5C827.327 399.333 824.077 401.083 819.91 402.75C815.91 404.417 811.16 405.833 805.66 407C800.327 408 794.493 408.5 788.16 408.5ZM749.41 332.25H814.41C814.41 319.083 811.91 309.25 806.91 302.75C801.91 296.083 794.41 292.75 784.41 292.75C773.577 292.75 765.243 296.083 759.41 302.75C753.577 309.25 750.243 319.083 749.41 332.25ZM880.676 407.5C874.009 407.5 870.676 405.667 870.676 402V281.25C870.676 277.583 874.009 275.75 880.676 275.75H881.676C888.342 275.75 891.676 277.583 891.676 281.25V402C891.676 405.667 888.342 407.5 881.676 407.5H880.676ZM881.176 258.25C877.176 258.25 874.342 257.75 872.676 256.75C871.009 255.75 870.176 254.417 870.176 252.75V237.75C870.176 236.083 871.009 234.75 872.676 233.75C874.342 232.75 877.176 232.25 881.176 232.25C885.176 232.25 888.009 232.75 889.676 233.75C891.342 234.583 892.176 235.917 892.176 237.75V252.75C892.176 254.417 891.342 255.75 889.676 256.75C888.009 257.75 885.176 258.25 881.176 258.25ZM928.088 460C925.921 460 923.338 459.833 920.338 459.5C917.505 459.167 915.088 458.417 913.088 457.25C910.921 456.25 909.838 454.667 909.838 452.5C909.838 451.833 910.088 450.5 910.588 448.5C910.921 446.667 911.505 444.917 912.338 443.25C913.005 441.583 913.921 440.75 915.088 440.75C916.255 440.75 917.421 440.917 918.588 441.25C919.755 441.75 922.171 442 925.838 442C929.505 442 932.671 440.75 935.338 438.25C938.005 435.917 939.338 431 939.338 423.5V281.25C939.338 277.583 942.671 275.75 949.338 275.75H950.338C957.005 275.75 960.338 277.583 960.338 281.25V425.5C960.338 432.667 959.255 438.5 957.088 443C954.921 447.667 952.171 451.167 948.838 453.5C945.505 456 942.005 457.667 938.338 458.5C934.671 459.5 931.255 460 928.088 460ZM949.838 258.25C945.838 258.25 943.005 257.75 941.338 256.75C939.671 255.75 938.838 254.417 938.838 252.75V237.75C938.838 236.083 939.671 234.75 941.338 233.75C943.005 232.75 945.838 232.25 949.838 232.25C953.838 232.25 956.671 232.75 958.338 233.75C960.005 234.583 960.838 235.917 960.838 237.75V252.75C960.838 254.417 960.005 255.75 958.338 256.75C956.671 257.75 953.838 258.25 949.838 258.25ZM1015.93 407.5C1009.26 407.5 1005.93 405.667 1005.93 402V281.25C1005.93 277.583 1009.26 275.75 1015.93 275.75H1016.93C1023.6 275.75 1026.93 277.583 1026.93 281.25V402C1026.93 405.667 1023.6 407.5 1016.93 407.5H1015.93ZM1016.43 258.25C1012.43 258.25 1009.6 257.75 1007.93 256.75C1006.26 255.75 1005.43 254.417 1005.43 252.75V237.75C1005.43 236.083 1006.26 234.75 1007.93 233.75C1009.6 232.75 1012.43 232.25 1016.43 232.25C1020.43 232.25 1023.26 232.75 1024.93 233.75C1026.6 234.583 1027.43 235.917 1027.43 237.75V252.75C1027.43 254.417 1026.6 255.75 1024.93 256.75C1023.26 257.75 1020.43 258.25 1016.43 258.25ZM1072.09 407.5C1065.43 407.5 1062.09 405.667 1062.09 402V385.5C1062.09 381.833 1065.43 380 1072.09 380H1076.34C1083.01 380 1086.34 381.833 1086.34 385.5V402C1086.34 405.667 1083.01 407.5 1076.34 407.5H1072.09ZM1112.65 407.5C1105.49 407.5 1101.9 406.083 1101.9 403.25C1101.9 402.75 1101.99 402.25 1102.15 401.75C1102.32 401.083 1102.49 400.417 1102.65 399.75L1161.9 235.5C1162.74 232.833 1166.07 231.5 1171.9 231.5H1179.15C1184.99 231.5 1188.32 232.833 1189.15 235.5L1248.4 400C1248.9 401.167 1249.15 402.25 1249.15 403.25C1249.15 406.083 1245.57 407.5 1238.4 407.5H1236.4C1233.9 407.5 1231.74 407.25 1229.9 406.75C1228.07 406.083 1226.9 405 1226.4 403.5L1211.65 362.5H1138.15L1123.9 403.5C1123.4 405 1122.24 406.083 1120.4 406.75C1118.57 407.25 1116.4 407.5 1113.9 407.5H1112.65ZM1145.15 343.5H1204.9L1178.65 269C1177.99 266.667 1177.4 264.25 1176.9 261.75C1176.4 259.25 1175.99 256.833 1175.65 254.5H1175.15C1174.65 256.833 1174.07 259.25 1173.4 261.75C1172.9 264.25 1172.24 266.667 1171.4 269L1145.15 343.5ZM1286.47 407.5C1279.81 407.5 1276.47 405.667 1276.47 402V237C1276.47 233.333 1279.81 231.5 1286.47 231.5H1288.47C1295.14 231.5 1298.47 233.333 1298.47 237V402C1298.47 405.667 1295.14 407.5 1288.47 407.5H1286.47Z" fill="white"/>
</svg>
\" alt=\"Keiji.AI\" width=\"300\"/> </center>\n",
        "\n",
        "TrialMind by Keiji.AI is a Generative AI assistant designed to support several Clinical Trial operations, research, and analytics activities. This notebook is used to demonstrate the Patient Trial Analytics Agent instantieted for the Medidata Simulants product.\n",
        "\n",
        "Find out more at: https://keiji.ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Y3pRVWB_0X"
      },
      "source": [
        "# Suggested Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eeabEYPCydq"
      },
      "source": [
        "Some suggested questions you can ask our agent:\n",
        "\n",
        "Co-pilot mode:\n",
        "- in python, how would I display a summary of the data actually contained in ADCM\n",
        "- Plot the top 10 most frequent medications\n",
        "- show me the patient journey with just the study treatment start and end\n",
        "- plot overall survival\n",
        "- plot progression free survival\n",
        "- Can you generate the timeline plots for all subjects, keep pre-conditioning therapies and ICANs?\n",
        "- Please find the adverse effects that are related to Immune effector cell-associated neurotoxicity syndrome (ICANS)\n",
        "- Suppose I have a trial with the phase 2 outcome measure Objective Response Rate. How would I determine that?\n",
        "- Could you help me find the adverse effects that are related to Immune effector cell-associated neurotoxicity syndrome (ICANS)?\n",
        "- Could you write a python script to find the optimal pre-conditioning regimens to minimize ICANs? Use logistic regression to measure feature importance.\n",
        "\n",
        "Non-co-pilot (vanilla) mode:\n",
        "- I want to perform an analysis on the dataset that highlights the endpoints: overall response rate, overall survival or progression free survival. What can I learn from this kind of analysis?\n",
        "- I have a trial with the phase 2 outcome measure Objective Response Rate. How would I determine that quantity? My dataset is based on CDISC ADaM|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvoiA3JV5aZ3"
      },
      "source": [
        "# Import Dataset and Perform Standard Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKSMOOVj9cbO",
        "outputId": "41cc08a9-86e5-4844-95d9-c616a39c906d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.11.3)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.6.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.6.6)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines) (0.18.3)\n",
            "Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->lifelines) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lifelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9el7kjH5SHh",
        "outputId": "04ab80d9-b34e-4fa7-b769-93629df608b7"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bc01658c5c69>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0madsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'anonymized_adsl.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0madlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'anonymized_adlbmi.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0madvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'anonymized_advs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/anonymized_adsl.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "import random\n",
        "import scipy.interpolate\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess as  sm_lowess\n",
        "import datetime\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# load the data\n",
        "path = \"/content/\"\n",
        "\n",
        "adsl = pd.read_csv(path+'anonymized_adsl.csv', index_col=0)\n",
        "adlb = pd.read_csv(path+'anonymized_adlbmi.csv')\n",
        "advs = pd.read_csv(path+'anonymized_advs.csv')\n",
        "adcm = pd.read_csv(path+'anonymized_adcm.csv')\n",
        "adrs = pd.read_csv(path+'anonymized_adrs.csv')\n",
        "adpr = pd.read_csv(path+'anonymized_adpr.csv')\n",
        "adae = pd.read_csv(path+'anonymized_adae.csv')\n",
        "adex = pd.read_csv(path+'anonymized_adex.csv')\n",
        "admh = pd.read_csv(path+'anonymized_admh.csv')\n",
        "\n",
        "## Predefined processing functions\n",
        "def multitimeline(df_dict: dict,\n",
        "                  common_uid: str = 'USUBJID',\n",
        "                  uid_select: pd.NA = pd.NA,\n",
        "                  order_by: str = 'x_start',\n",
        "                  bin_correct_ends: bool = True,\n",
        "                  days_to_autoadd: int = 1,\n",
        "                  bin_create_dates_from_days: bool = True):\n",
        "    '''\n",
        "    Ingests a list of event-level datasets and creates a single, unified timeline.\n",
        "\n",
        "    Args:\n",
        "        df_dict (dict): Dictionary containing event-level datasets.\n",
        "        common_uid (str, optional): Column name for common unique identifier. Defaults to 'USUBJID'.\n",
        "        uid_select (pd.NA, optional): Value to filter the `common_uid` column for. Defaults to pd.NA.\n",
        "        order_by (str, optional): Column name to order the timeline. Defaults to 'x_start'.\n",
        "        bin_correct_ends (bool, optional): Whether to correct the end values. Defaults to True.\n",
        "        days_to_autoadd (int, optional): Number of days to add for automatic padding. Defaults to 1.\n",
        "        bin_create_dates_from_days (bool, optional): Whether to tranforms days into dates using today as the reference point. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the timeline data frame and the timeline plot.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `df_dict` is not a dictionary or if it doesn't contain required keys.\n",
        "        KeyError: If any required keys are missing in `df_dict`.\n",
        "    '''\n",
        "\n",
        "    # Validate input dictionary\n",
        "    if not isinstance(df_dict, dict):\n",
        "        logging.error(\"The input 'df_dict' must be a dictionary.\")\n",
        "        raise ValueError(\"Input 'df_dict' must be a dictionary.\")\n",
        "\n",
        "    required_keys = ['df', 'x_start', 'x_end', 'label']\n",
        "    missing_keys = [key for key in required_keys if key not in df_dict.get(next(iter(df_dict)), {})]\n",
        "    if missing_keys:\n",
        "        logging.error(f\"Missing required key(s) in 'df_dict': {', '.join(missing_keys)}\")\n",
        "        raise KeyError(f\"Missing required key(s) in 'df_dict': {', '.join(missing_keys)}\")\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for key, data in df_dict.items():\n",
        "        # Extract required columns from data frame and add color column if available\n",
        "        temp_df = data['df'][[common_uid, data['x_start'], data['x_end'], data['label']]]\n",
        "        if 'color' in data:\n",
        "            temp_df['color'] = key + ':' + data['color'].astype(str)\n",
        "        else:\n",
        "            temp_df['color'] = key + ':'\n",
        "\n",
        "        # Filter data frame based on common_uid and drop rows with missing values\n",
        "        temp_df = temp_df[temp_df[common_uid] == uid_select].dropna()\n",
        "\n",
        "        temp_df.columns = [common_uid, 'x_start', 'x_end', 'label', 'color']\n",
        "\n",
        "        if temp_df.empty:\n",
        "            continue\n",
        "\n",
        "        if bin_create_dates_from_days:\n",
        "            now = datetime.datetime.now()\n",
        "            # Convert time variables from days to dates using current date as reference\n",
        "            temp_df['x_start'] = now + pd.to_timedelta(temp_df['x_start'], unit='D')\n",
        "            temp_df['x_end'] = now + pd.to_timedelta(temp_df['x_end'], unit='D')\n",
        "        else:\n",
        "            # Convert x_start and x_end columns to datetime if they are not already\n",
        "            temp_df['x_start'] = pd.to_datetime(temp_df['x_start'])\n",
        "            temp_df['x_end'] = pd.to_datetime(temp_df['x_end'])\n",
        "\n",
        "        # Create a new column, x_start+1, by adding days_to_autoadd to x_start\n",
        "        temp_df['x_start+1'] = temp_df['x_start'] + pd.DateOffset(days=days_to_autoadd)\n",
        "        # Correct x_end values by taking the maximum between x_end and x_start+1\n",
        "        temp_df['x_end corr'] = temp_df[['x_end', 'x_start+1']].max(axis=1)\n",
        "\n",
        "        dfs.append(temp_df)\n",
        "\n",
        "    # Concatenate the data frames from all datasets\n",
        "    if len(dfs) == 0:\n",
        "        return None, None\n",
        "    else:\n",
        "        tl_df = pd.concat(dfs).sort_values(order_by)[['x_start', 'x_end corr', 'label', 'color']].dropna()\n",
        "\n",
        "    if tl_df.empty:\n",
        "        logging.warning(\"The timeline data frame is empty.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # Generate the timeline plot using the timeline data frame\n",
        "        if bin_correct_ends:\n",
        "            plot = px.timeline(tl_df, x_start='x_start', x_end='x_end corr', y=\"label\", color='color')\n",
        "        else:\n",
        "            plot = px.timeline(tl_df, x_start='x_start', x_end='x_end', y=\"label\", color='color')\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error occurred while generating the timeline plot.\")\n",
        "        logging.error(str(e))\n",
        "        return None, None\n",
        "\n",
        "    return tl_df.sort_values(by = \"x_start\"), plot\n",
        "\n",
        "\n",
        "def bor_from_timeline(tl_df_: pd.DataFrame,\n",
        "                      uid: str,\n",
        "                      start_event: str = '0. 1L Treatment:',\n",
        "                      overall_response_event: str = '1. Overall Response:',\n",
        "                      end_event: str = '3. Last Date:',\n",
        "                      bor_rank: list = ['CR', 'CMR', 'CRH', 'IRCR', 'CRi', 'CRI', 'REMISSION', 'PR/CR',\n",
        "                                        'IRPR', 'PR', 'PMR', 'SD', 'IRSD', 'NO RESPONSE', 'SMD',\n",
        "                                        'PD', 'RELAPSED DISEASE', 'RELAPSE', 'PMD', 'IRPD'],\n",
        "                      bor_to_drop: list = ['NE', 'U', 'UNKNOWN', 'BFBM (BLAST-FREE HYPOPLASTIC OR APLASTIC BM)',\n",
        "                                           'NR', 'Missing/Unknown', 'NON-CR/NON-PD'],\n",
        "                      end_of_bor: list = ['PD', 'RELAPSED DISEASE', 'RELAPSE', 'PMD', 'IRPD'],\n",
        "                      positive_response: list = ['CR', 'CMR', 'CRH', 'IRCR', 'CRi', 'CRI',\n",
        "                                                 'REMISSION', 'IRPR', 'PR', 'PMR', 'PR/CR'],\n",
        "                      next_tx_end_of_bor = True):\n",
        "  '''\n",
        "  Creates best overall response dataframe based on individual timeline. Intended to be called subject by subject by providing one `uid` at a time.\n",
        "\n",
        "  Args:\n",
        "        tl_df (pd.DataFrame): timeline dataframe for a patient.\n",
        "        uid (str): patient identifier for which tl_df was provided.\n",
        "        start_event (str, optional): category in 'color' column of `tl_df` representing the treatment event. Defaults to '0. 1L Treatment:'.\n",
        "        overall_response_event (str, optional): category in 'color' column of `tl_df` representing the overall response. Defaults to '1. Overall Response:'.\n",
        "        end_event (str, optional): category in 'color' column of `tl_df` representing the end event. Defaults to '3. Last Date:'.\n",
        "        bor_rank (list, optional): list of responses ordered from \"best\" to \"worst.\n",
        "        bor_to_drop (list, optional): list of responses to ignore.\n",
        "        end_of_bor (list, optional): list of responses that represent the end of best-overall-response period.\n",
        "        positive_response (list, optional): list of responses that are positive response to the treatment.\n",
        "        death_end_of_bor (bool, optional): whether to include death as an end of BOR event. Defaults to True.\n",
        "        next_tx_end_of_bor (bool, optional): whether to include new treatment lines as an end of BOR event. Defaults to True.\n",
        "        value_new_tx_filter (list, optional): list of the medications considered new lines of tx. Defaults to [].\n",
        "\n",
        "  Returns:\n",
        "        pd.DataFrame: dataframe with calculated (best) overall response and associated timelines\n",
        "  '''\n",
        "\n",
        "  tl_df = tl_df_.copy()\n",
        "\n",
        "  bor_df = pd.DataFrame(index=[uid], columns = ['FLAG_TTR', 'OR_TTR', 'TTR', 'BOR',\n",
        "                                                'TTE_BOR', 'FLAG_END_OF_BOR',\n",
        "                                                'END_OF_BOR_TYPE', 'FLAG_DOR', 'TTE_DOR',\n",
        "                                                'DOR_BOR', 'FLAG_PFS','TTE_PFS',\n",
        "                                                'FLAG_OS', 'TTE_OS'])\n",
        "\n",
        "  zero_date = tl_df.loc[tl_df.loc[:, 'color'] == start_event, 'x_start'].min()\n",
        "  last_date = tl_df.loc[tl_df.loc[:, 'color'] == end_event, 'x_start'].max()\n",
        "\n",
        "\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: SET UP OF THE TIMELINE DATAFRAME ###\n",
        "  #################################################\n",
        "\n",
        "  # If death is to be included as an end of BOR event\n",
        "  tl_death_idx = tl_df.color.str.contains(\"DEATH\", case=False)\n",
        "  tl_df.loc[tl_death_idx, \"label\"] = \"Death\"\n",
        "  tl_df.loc[tl_death_idx, \"color\"] = overall_response_event\n",
        "\n",
        "  end_of_bor = end_of_bor + [\"Death\"]\n",
        "  bor_rank = bor_rank + [\"Death\"]\n",
        "\n",
        "\n",
        "  bor_dict = {bor_rank[i]: i for i in range(len(bor_rank))}\n",
        "  for bor in bor_to_drop:\n",
        "    bor_dict[bor] = np.nan\n",
        "\n",
        "  # Assign rank to response events according to bor_rank\n",
        "  tl_df.loc[tl_df.label.isin(bor_rank), 'label_rank'] = tl_df.loc[tl_df.label.isin(bor_rank), 'label'].replace(bor_dict)\n",
        "\n",
        "  # If new tx lines post CART are to be included as a censoring event\n",
        "  if next_tx_end_of_bor:\n",
        "    list_new_tx = list(tl_df[tl_df.color.str.contains(\"TREATMENT\", case = False)].label)\n",
        "    end_of_bor = list(set(end_of_bor).union(set(list_new_tx)))\n",
        "    tl_new_tx_idx = tl_df.label.isin(list_new_tx)\n",
        "    tl_df.loc[tl_new_tx_idx, \"color\"] = overall_response_event\n",
        "  else:\n",
        "    list_new_tx = []\n",
        "\n",
        "  # If new lines of tx are to be included as censoring events\n",
        "  if next_tx_end_of_bor:\n",
        "    tl_new_tx_idx = tl_df.label.isin(list_new_tx)\n",
        "    if tl_new_tx_idx.sum() > 0:\n",
        "      new_tx_date = tl_df.loc[tl_new_tx_idx, \"x_start\"].min()\n",
        "    else:\n",
        "      new_tx_date = last_date + pd.DateOffset(days = 3)\n",
        "\n",
        "  else:\n",
        "    new_tx_date = last_date + pd.DateOffset(days = 3)\n",
        "\n",
        "  # Censoring date is the first of the last follow up date and new tx date\n",
        "  last_censored_date = min(new_tx_date, last_date)\n",
        "\n",
        "  # If the patient experienced death\n",
        "  if tl_df.loc[tl_death_idx, \"label\"].shape[0] > 0:\n",
        "    death_date = tl_df.loc[tl_death_idx, \"x_start\"].min()\n",
        "    # Including death as a censoring event\n",
        "    last_censored_date = min(last_censored_date, death_date)\n",
        "  else:\n",
        "    death_date = last_date + pd.DateOffset(days = 3)\n",
        "\n",
        "  # Checking if the patient has a record of SD before new tx line, death or end of follow up\n",
        "  tl_SD_idx = tl_df[(tl_df.x_start <= last_censored_date)&(\n",
        "      tl_df.x_start >= zero_date)].label.astype(str).str.contains(\"SD\", case = False)\n",
        "  if tl_SD_idx.sum():\n",
        "    print(uid)\n",
        "    last_SD_date = tl_df[(tl_df.x_start <= last_censored_date)&(\n",
        "      tl_df.x_start >= zero_date) & (\n",
        "          tl_df.label.astype(str).str.contains(\"SD\", case = False))].loc[tl_SD_idx, \"x_start\"].max()\n",
        "    last_SD_day = (last_SD_date - zero_date).days\n",
        "    flag_SD_before_censoring = True\n",
        "\n",
        "  else:\n",
        "    flag_SD_before_censoring = False\n",
        "    last_SD_date = last_censored_date\n",
        "    last_SD_day = (last_SD_date - zero_date).days\n",
        "\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: OVERALL SURVIVAL CALCULATION ###\n",
        "  #################################################\n",
        "\n",
        "  last_follow_up = (last_censored_date  - zero_date).days\n",
        "\n",
        "  # Calculating the OS if the patient dies before the censoring event\n",
        "  if (tl_df.loc[tl_death_idx, \"label\"].shape[0] > 0):\n",
        "    death_days = (death_date - zero_date).days\n",
        "\n",
        "    if  death_date <= last_censored_date:\n",
        "      bor_df.loc[uid, 'TTE_OS'] = death_days\n",
        "      flag_OS = True\n",
        "\n",
        "    else:\n",
        "      bor_df.loc[uid, 'TTE_OS'] = last_follow_up\n",
        "      flag_OS = False\n",
        "\n",
        "\n",
        "  else:\n",
        "  # Calculating the OS if no death\n",
        "    bor_df.loc[uid, 'TTE_OS'] = last_follow_up\n",
        "    flag_OS = False\n",
        "\n",
        "  bor_df.loc[uid, 'FLAG_OS'] = flag_OS\n",
        "\n",
        "\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: BOR CALCULATION ###\n",
        "  #################################################\n",
        "\n",
        "  # Define the start of best overall response (BOR)\n",
        "  bor_start_data = tl_df.copy()\n",
        "  bor_start_data = bor_start_data.loc[(bor_start_data.loc[:, 'color'] == overall_response_event) &\n",
        "   (bor_start_data.loc[:, 'x_start'] > zero_date) &\n",
        "   (bor_start_data.loc[:, 'x_start'] <= last_censored_date), :] # select responses between the start date and censor date\n",
        "\n",
        "  bor_start_data = bor_start_data.sort_values(['label_rank', 'x_start']) # sort responses from best to worst and by date\n",
        "  bor_start_data = bor_start_data.dropna() # remove any missing responses\n",
        "\n",
        "  if bor_start_data.shape[0] > 0: # check to make sure there is at least one response recorded\n",
        "    bor_start_data = bor_start_data.iloc[0, :]\n",
        "    bor_start_date = bor_start_data.loc['x_start']\n",
        "    bor_df.loc[uid, 'TTE_BOR'] = (bor_start_date - zero_date).days # time to event for the Best Overall Response\n",
        "    bor_df.loc[uid, 'BOR']     = bor_start_data.loc['label'] # what is the best overall response?\n",
        "  else:\n",
        "    return bor_df\n",
        "\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: OR CALCULATION ###\n",
        "  #################################################\n",
        "\n",
        "  # Define the start of the response (CR or PR)\n",
        "  or_start_data = tl_df.copy()\n",
        "  or_start_data = or_start_data.loc[(or_start_data.loc[:, 'color'] == overall_response_event) &\n",
        "    (or_start_data.loc[:, 'x_start'] > zero_date) & # select responses after the start date\n",
        "    (or_start_data.loc[:, 'x_start'] <= last_censored_date), :] # select responses before the censoring date\n",
        "  or_start_data = or_start_data[or_start_data[\"label\"].isin(positive_response)] # Filtering only for the PR or CR events\n",
        "\n",
        "\n",
        "  or_start_data = or_start_data.sort_values(['x_start']) # sort responses by date\n",
        "  or_start_data = or_start_data.dropna() # remove any missing responses\n",
        "\n",
        "  if or_start_data.shape[0] > 0: # check to make sure there is at least one positive response recorded\n",
        "    or_start_data = or_start_data.iloc[0,:]\n",
        "    or_start_date = or_start_data.loc['x_start']\n",
        "    bor_df.loc[uid, 'TTR'] = (or_start_date - zero_date).days # time to event for the first of any positive response (PR or CR)\n",
        "    bor_df.loc[uid, 'OR_TTR'] = or_start_data.loc['label'] # which type of response is the first positive response?\n",
        "    bor_df.loc[uid, 'FLAG_TTR'] = True\n",
        "\n",
        "    bor_end_data = tl_df.loc[(tl_df.loc[:,'color'] == overall_response_event) &\n",
        "        (tl_df.loc[:, 'label'].isin(end_of_bor)) &\n",
        "        (tl_df.loc[:, 'x_start'] >  bor_start_date), :]\n",
        "        # If the patient experiences a positive response,\n",
        "        # look at the end of response events only after the date of the BOR (NOT including the date of BOR)\n",
        "  else:\n",
        "    bor_df.loc[uid, 'FLAG_TTR'] = False # No response recorded, Flag set to False\n",
        "\n",
        "    bor_end_data = tl_df.loc[(tl_df.loc[:, 'color'] == overall_response_event) &\n",
        "        (tl_df.loc[:, 'label'].isin(end_of_bor)) &\n",
        "        (tl_df.loc[:, 'x_start'] >=  bor_start_date), :]\n",
        "        # If the patient does not experience a positive response,\n",
        "        # look at the end of response events only after the date of the BOR (including the date of BOR).\n",
        "        # Ex, if the BOR is a PD, measured on day 10, day 10 would be the day of the end of BOR\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: PFS CALCULATION ###\n",
        "  #################################################\n",
        "\n",
        "  if bor_end_data.shape[0] > 0: # if the patient experiences an end of BOR event\n",
        "    bor_end_data = bor_end_data.sort_values(['x_start', 'color']) # sort responses by date (note that *any* end_of_bor response is sufficient to determine duration of BOR, best or worst does not matter)\n",
        "    bor_end_data = bor_end_data.iloc[0, :]\n",
        "    tte_end_bor  = (bor_end_data.loc['x_start'] - zero_date).days # Number of days from start day to end of BOR\n",
        "    bor_df.loc[uid, 'END_OF_BOR_TYPE'] = bor_end_data.loc['label']\n",
        "\n",
        "    ## Censoring rules for PFS and END_OF_BOR\n",
        "\n",
        "    # If the end of END_OF_BOR_TYPE is within the events to be considered as end_of_bor\n",
        "    if bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list(set(end_of_bor) - set(list_new_tx) - set([\"Death\"])):\n",
        "      flag_pfs_end_bor = True\n",
        "\n",
        "    # If the patient receives new tx or experiences death, checking for any SD record before, and taking the last record:\n",
        "    elif (bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list_new_tx + [\"Death\"]):\n",
        "\n",
        "      if (last_SD_day < tte_end_bor) & ( last_SD_date > bor_start_date):\n",
        "        print(uid)\n",
        "        tte_end_bor = last_SD_day\n",
        "        bor_df.loc[uid, 'END_OF_BOR_TYPE'] = \"SD\"\n",
        "        flag_pfs_end_bor = False\n",
        "\n",
        "      # If the end of END_OF_BOR_TYPE is a new treatment line\n",
        "      elif bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list_new_tx:\n",
        "        flag_pfs_end_bor = False\n",
        "\n",
        "      # If the end of END_OF_BOR_TYPE is death\n",
        "      elif bor_df.loc[uid, 'END_OF_BOR_TYPE'] == \"Death\":\n",
        "        flag_pfs_end_bor = True\n",
        "\n",
        "  # If no END_OF_BOR_TYPE recorded, the patient is censored on the last follow up date\n",
        "  else:\n",
        "    flag_pfs_end_bor = False\n",
        "    tte_end_bor = (last_censored_date  - zero_date).days\n",
        "\n",
        "    # If the patient is censored, checking for any SD record before the censoring, and taking the last record:\n",
        "    if (last_SD_day < tte_end_bor)& ( last_SD_date > bor_start_date):\n",
        "        print(uid)\n",
        "        tte_end_bor = last_SD_day\n",
        "        bor_df.loc[uid, 'END_OF_BOR_TYPE'] = \"SD\"\n",
        "\n",
        "\n",
        "  bor_df.loc[uid, 'TTE_PFS'] = tte_end_bor # PFS: number of days leading to the end of BOR event.\n",
        "\n",
        "\n",
        "  #################################################\n",
        "  ### SECTION: DOR CALCULATION ###\n",
        "  #################################################\n",
        "\n",
        "  if bor_end_data.shape[0] > 0:\n",
        "    ## Censoring rules for DOR\n",
        "    if bor_df.loc[uid,'BOR'] in positive_response: # If the patient experiences a positive response\n",
        "\n",
        "      # If the end of END_OF_BOR_TYPE is within the events to be considered as end_of_bor\n",
        "      if bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list(set(end_of_bor) - set(list_new_tx) - set([\"Death\"])):\n",
        "        flag_dor = True\n",
        "\n",
        "        # If the patient receives new tx or experiences death, checking for any SD record before, and taking the last record:\n",
        "      elif (bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list_new_tx + [\"Death\"]):\n",
        "\n",
        "        # Censoring at last SD record\n",
        "        if (last_SD_day < tte_end_bor) & ( last_SD_date > bor_start_date):\n",
        "          flag_dor = False\n",
        "\n",
        "        # If the end of END_OF_BOR_TYPE is a new treatment line\n",
        "        elif bor_df.loc[uid, 'END_OF_BOR_TYPE'] in list_new_tx:\n",
        "          flag_dor = False\n",
        "\n",
        "        # If the end of END_OF_BOR_TYPE is death\n",
        "        elif bor_df.loc[uid, 'END_OF_BOR_TYPE'] == \"Death\":\n",
        "          flag_dor = True\n",
        "\n",
        "      # If no END_OF_BOR_TYPE recorded, the patient is censored on the last follow up date\n",
        "      else:\n",
        "        flag_dor = False\n",
        "\n",
        "    else: # No positive response recorded\n",
        "      flag_dor = False\n",
        "\n",
        "  else: # No end of BOR event, the end date is the censoring date\n",
        "    flag_dor = False\n",
        "\n",
        "\n",
        "  bor_df.loc[uid, 'FLAG_PFS'] = flag_pfs_end_bor\n",
        "  bor_df.loc[uid, 'FLAG_END_OF_BOR'] = flag_pfs_end_bor\n",
        "  bor_df.loc[uid, 'FLAG_DOR'] = flag_dor\n",
        "\n",
        "\n",
        "  if bor_df.loc[uid, 'BOR'] in positive_response: # if the patient experiences a positive response, we can calculate the DOR\n",
        "    bor_df.loc[uid, 'DOR_BOR'] = tte_end_bor - bor_df.loc[uid, 'TTE_BOR'] # Time in days from BOR to an end of BOR event (e.g. PD, Relapse, Death)\n",
        "    bor_df.loc[uid, 'TTE_DOR'] = tte_end_bor - bor_df.loc[uid, 'TTR'] # Duration of Response\n",
        "\n",
        "\n",
        "  return bor_df\n",
        "\n",
        "# List of new treatments medications that are to be considered as censoring events\n",
        "value_new_tx_censor = []\n",
        "\n",
        "\n",
        "# Defaults values for new treatments medications that are to be considered as censoring events\n",
        "if value_new_tx_censor == []: value_new_tx_censor = list(set(adcm.loc[adcm.CMCAT=='POST ANTI-CANCER THERAPY',:].CMDECOD))\n",
        "\n",
        "tl_dict = { 'START' : {'df'      : adsl,\n",
        "                                    'x_start' :'PJSTDY',\n",
        "                                    'x_end'   :'PJENDY',\n",
        "                                    'label'   :'TRTARM'},\n",
        "\n",
        "            'EVENTS - RESPONSES': {'df'      : adrs.loc[adrs.APARAM.str.contains('Overall Response'),:],\n",
        "                                    'x_start' :'ADY',\n",
        "                                    'x_end'   :'ADY',\n",
        "                                    'label'   :'AVALC'},\n",
        "\n",
        "\n",
        "            'EVENTS - DEATH'           : {'df'     : adsl,\n",
        "                                    'x_start':'DTHDY',\n",
        "                                    'x_end'  :'DTHDY',\n",
        "                                    'label'  :'DTHDY'},\n",
        "\n",
        "            'LAST FOLLOW UP'        : {'df'     : adsl,\n",
        "                                    'x_start':'LASTPDY',\n",
        "                                    'x_end'  :'LASTPDY',\n",
        "                                    'label'  :'LASTPDY'},\n",
        "\n",
        "\n",
        "            'CENSORING - NEW TREATMENTS'  : {'df'      : adcm.loc[(adcm.CMCAT=='POST ANTI-CANCER THERAPY')\n",
        "                                                        & (adcm.CMDECOD.isin(value_new_tx_censor)),:],\n",
        "                                    'x_start' :'ASTDY',\n",
        "                                    'x_end'   :'AENDY',\n",
        "                                    'label'   :'CMDECOD'}\n",
        "\n",
        "\n",
        "          }\n",
        "\n",
        "#### Label ethg as either censoring or events\n",
        "\n",
        "start_event = 'START:'\n",
        "overall_response_event = 'EVENTS - RESPONSES:'\n",
        "end_event = 'LAST FOLLOW UP:'\n",
        "bor_rank   = ['CR','CMR','CRH','IRCR','CRi','CRI','REMISSION',\"PR/CR\",\n",
        "                                    'IRPR','PR','PMR','SD','IRSD','NO RESPONSE','SMD',\n",
        "                                    'PD','RELAPSED DISEASE','RELAPSE','PMD','IRPD']\n",
        "\n",
        "bor_to_drop = ['NE','U','UNKNOWN','BFBM (BLAST-FREE HYPOPLASTIC OR APLASTIC BM)', 'NR','Missing/Unknown','NON-CR/NON-PD']\n",
        "end_of_bor = ['PD','RELAPSED DISEASE','RELAPSE','PMD','IRPD']\n",
        "positive_response = ['CR','CMR','CRH','IRCR','CRi','CRI','REMISSION',\n",
        "                                    'IRPR','PR','PMR',\"PR/CR\"]\n",
        "next_tx_end_of_bor = True\n",
        "\n",
        "ct = 0\n",
        "bor_dfs = []\n",
        "tl_dfs  = []\n",
        "\n",
        "pt_ids  = adsl.USUBJID.unique()\n",
        "\n",
        "# currently this loop is not optimized for time, but it will produce one timeline dataframe & one best-overall-response vector for each subject\n",
        "for pt_id in pt_ids:\n",
        "    ct += 1\n",
        "    if ct % 100 == 1:\n",
        "      print(ct,'out of',len(pt_ids))\n",
        "\n",
        "    # creating the timeline for the specific patient\n",
        "    tl_df, fig = multitimeline(df_dict=tl_dict,\n",
        "                               common_uid='USUBJID',\n",
        "                               uid_select=pt_id,\n",
        "                               order_by='color',\n",
        "                               days_to_autoadd=10)\n",
        "\n",
        "    try:\n",
        "      bor_df = None\n",
        "\n",
        "      # From the timeline calculating the response events\n",
        "      bor_df = bor_from_timeline(tl_df_ =tl_df,\n",
        "                                 uid=pt_id ,\n",
        "                                 start_event=start_event,\n",
        "                                 overall_response_event=overall_response_event,\n",
        "                                 end_event=end_event,\n",
        "                                 bor_rank=bor_rank,\n",
        "                                 bor_to_drop=bor_to_drop,\n",
        "                                 end_of_bor=end_of_bor,\n",
        "                                 positive_response=positive_response,\n",
        "                                 next_tx_end_of_bor = next_tx_end_of_bor)\n",
        "      bor_dfs.append(bor_df)\n",
        "      tl_df.index = [(pt_id, i) for i in tl_df.reset_index().index]\n",
        "      tl_dfs.append(tl_df)\n",
        "    except:\n",
        "      print('ERROR', pt_id)\n",
        "      display(tl_df)\n",
        "      display(bor_df)\n",
        "      break\n",
        "\n",
        "bor_df        = pd.concat(bor_dfs)\n",
        "pt_journey_df = pd.concat(tl_dfs)\n",
        "\n",
        "pt_journey_df.index = pd.MultiIndex.from_tuples(pt_journey_df.index.tolist()) # generates a hierarchical index for the concatenated patient journey dataframe\n",
        "\n",
        "bor_df= bor_df[[\"FLAG_TTR\",'OR_TTR', 'TTR','BOR','TTE_BOR','FLAG_END_OF_BOR','END_OF_BOR_TYPE',\"FLAG_DOR\",'TTE_DOR',\n",
        "                'DOR_BOR','FLAG_PFS','TTE_PFS',\n",
        "                \"FLAG_OS\", \"TTE_OS\"]]\n",
        "\n",
        "\"|\".join(bor_df.columns.tolist())\n",
        "\n",
        "\"|\".join(pt_journey_df[\"color\"].value_counts().index.tolist())\n",
        "\n",
        "pt_journey_df\n",
        "\n",
        "print(pt_journey_df.head(5).to_markdown())\n",
        "\n",
        "adsl = bor_df.reset_index().rename(columns={\"index\":\"USUBJID\"}).merge(adsl, on=\"USUBJID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE-eLrK19lRf"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwD4iTOZ92TA"
      },
      "source": [
        "Paste Code Generated by TrialMind Patient Analysis Co-Pilot below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DYzeyH5s9-Hb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}